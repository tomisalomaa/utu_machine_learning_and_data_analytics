{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "ADA2021_exercise2_Tomi_Salomaa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW7akpvtXOH5"
      },
      "source": [
        "Tomi Salomaa <br>\n",
        "Student number  <br>\n",
        "February, 9, 2021  <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i78AGaC7XOH9"
      },
      "source": [
        "# Exercise 2 | TKO_2096 Application of Data Analysis 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKq57ykoXOH-"
      },
      "source": [
        "#### Prediction of the metal ion content from multi-parameter data <br>\n",
        "- Use K-Nearest Neighbor Regression with euclidean distance to predict total metal concentration (c_total), concentration of Cadmium (Cd) and concentration of Lead (Pb), for each sample using number of neighbors k = 3.<br> <br>\n",
        "\n",
        "    - You may use Nearest Neighbor Regression from https://scikit-learn.org/stable/modules/neighbors.html\n",
        "    - The data should be standarized using z-score.\n",
        "    - Implement your own Leave-One-Out cross-validation and calculate the C-index for each output (c_total, Cd, Pb). \n",
        "    - Implement your own Leave-Replicas-Out cross-validation and calculate the C-index for each output (c_total, Cd, Pb).\n",
        "    - Return your solution as a Jupyter Notebook file (include your full name in the file name).\n",
        "    - Submit to moodle your solution on ** Wednesday 10 of February** at the latest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es3AnpKbXOH-"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nDdJgltXOH_"
      },
      "source": [
        "#In this cell import all libraries you need. \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0fZBpOuQNeg"
      },
      "source": [
        "#!pip install -U scikit-learn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmIpIkWGTtj9",
        "outputId": "74f3ddb4-5b28-45ec-d535-b9e35d18c56a"
      },
      "source": [
        "print('Scikit-learn: {}'.format(sklearn.__version__))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scikit-learn: 0.24.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BcrK9O4DuNI"
      },
      "source": [
        "# upload dataset source file for google colab IDE\r\n",
        "#uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z3Cft2pXOH_"
      },
      "source": [
        "## Read and visualize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EanNNUn3XOIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d4fccb-a15b-4dcd-b20c-fbae7d9c69d2"
      },
      "source": [
        "# Reading the dataset\n",
        "water_data = pd.read_csv('Water_data.csv')\n",
        "\n",
        "# Presenting shape of the dataset\n",
        "print(\"Shape of 'water_data' dataset:\",water_data.shape)\n",
        "\n",
        "# Printing the first 5 rows\n",
        "print('\\nFirst 5 rows of the dataset')\n",
        "print('-'*45)\n",
        "print(water_data.head())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of 'water_data' dataset: (225, 6)\n",
            "\n",
            "First 5 rows of the dataset\n",
            "---------------------------------------------\n",
            "   c_total     Cd      Pb    Mod1  Mod2    Mod3\n",
            "0     2000  800.0  1200.0  126430  2604    6996\n",
            "1       35   14.0    21.0   20597   271  138677\n",
            "2       35   14.0    21.0   24566   269  161573\n",
            "3       35   35.0     0.0  105732   971  132590\n",
            "4      100   20.0    80.0   57774  5416   93798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9cezMhwXOIA"
      },
      "source": [
        "#### To show understanding of the data, answer the following questions:\n",
        "- How many different mixtures of Cadmium (Cd) and Lead (Pb) were measured? <br>\n",
        "- How many total concentrations (c_total) were measured? <br>\n",
        "- How many mixtures have less than 4 replicas? <br>\n",
        "- How many mixtures have 4 or more replicas? Print out c_total, Cd and Pb for those concentrations.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2ZPfWJMXOIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63652369-d235-4092-a485-104eee84394d"
      },
      "source": [
        "#In this cell write the code to answer the previous questions and print the answers.\r\n",
        "\r\n",
        "# Finding out the amount of unique mixtures of Cd and Pb measured\r\n",
        "cd_and_pb_col = water_data[['c_total','Cd','Pb']]\r\n",
        "duplicates_mixtures = cd_and_pb_col.groupby(cd_and_pb_col.columns\r\n",
        "                                            .tolist(),as_index=False).size()\r\n",
        "all_dup_rows_mix = duplicates_mixtures['size'].sum()\r\n",
        "print('Amount of mixtures in the dataset')\r\n",
        "print('-'*35)\r\n",
        "print('Amount of unique Cd and Pb mixture combinations:',\r\n",
        "      len(duplicates_mixtures.index))\r\n",
        "print('Amount of rows with all duplicates combined:',all_dup_rows_mix)\r\n",
        "if all_dup_rows_mix < len(water_data):\r\n",
        "  print('Unique combinations and their duplicates amount to',all_dup_rows_mix)\r\n",
        "  print('which is less than the total of',len(water_data),\r\n",
        "        'rows observed in the original dataset.')\r\n",
        "  print('Thus, the amount of unique mixtures in the dataset amounts to:',\r\n",
        "        len(duplicates_mixtures.index) + len(water_data) - all_dup_rows_mix)\r\n",
        "  print('(Unique mixtures without duplicates:',\r\n",
        "        len(water_data) - all_dup_rows_mix,')')\r\n",
        "elif all_dup_rows_mix == len(water_data):\r\n",
        "  print('Unique combinations and their duplicates fill the original dataset exactly.')\r\n",
        "  print('Thus, the amount of observed uniques (',len(duplicates_mixtures.index),\r\n",
        "        ') is the amount of different mixtures.')\r\n",
        "else:\r\n",
        "  print('This cannot be!')\r\n",
        "\r\n",
        "# Finding out the amount of total concentrations measured\r\n",
        "print('\\n')\r\n",
        "has_null_values = water_data['c_total'].isnull().values.any()\r\n",
        "if has_null_values:\r\n",
        "  print('Null c_total values:',has_null_values)\r\n",
        "  print('Null values in total:',water_data['your column name'].isnull().sum())\r\n",
        "else:\r\n",
        "  c_total_col = water_data[['c_total']]\r\n",
        "  duplicates_c_tot = c_total_col.groupby(c_total_col.columns\r\n",
        "                                   .tolist(),as_index=False).size()\r\n",
        "  c_total_zero = duplicates_c_tot.loc[duplicates_c_tot['c_total'] == 0]\r\n",
        "  all_dup_rows_c_tot = duplicates_c_tot['size'].sum()\r\n",
        "  print('Total concentrations measured')\r\n",
        "  print('-'*30)\r\n",
        "  print('The dataset does not contain null c_total values.')\r\n",
        "  print('     Total concentration value greater than 0:',\r\n",
        "        all_dup_rows_c_tot-c_total_zero['size'][0])\r\n",
        "  print('     Total concentration value 0:',c_total_zero['size'][0])\r\n",
        "  print('As mentioned in the \"Metal ion concentration data\" lecture by Perez,',\r\n",
        "        '\\na value of 0 can be assumed to be a result indicating pure tap water.',\r\n",
        "        '\\nThus, the measured amount of total concentrations is',\r\n",
        "        all_dup_rows_c_tot)\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "print('More info on duplicate data')\r\n",
        "print('-'*30)\r\n",
        "\r\n",
        "# Finding out mixtures with less than 4 duplicates\r\n",
        "less_than_four_duplicates = 0\r\n",
        "for i in range(len(duplicates_mixtures)):\r\n",
        "  if duplicates_mixtures.iloc[i]['size'] < 4:\r\n",
        "    less_than_four_duplicates += 1\r\n",
        "print('Mixtures with less than 4 duplicates:',less_than_four_duplicates)\r\n",
        "print('Mixtures with 4 or more duplicates:',\r\n",
        "      len(duplicates_mixtures.index)-less_than_four_duplicates)\r\n",
        "\r\n",
        "# Presenting mixtures with 4 or more duplicates\r\n",
        "# (c_total, Cd and Pb for those concentrations)\r\n",
        "new_dup_mix = duplicates_mixtures.rename({'size':'duplicates'},axis=1)\r\n",
        "print('\\n')\r\n",
        "print('Mixtures with 4 or more duplicates are:\\n')\r\n",
        "print(new_dup_mix.loc[new_dup_mix['duplicates']>=4]\r\n",
        "      .to_string(index=False))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amount of mixtures in the dataset\n",
            "-----------------------------------\n",
            "Amount of unique Cd and Pb mixture combinations: 67\n",
            "Amount of rows with all duplicates combined: 225\n",
            "Unique combinations and their duplicates fill the original dataset exactly.\n",
            "Thus, the amount of observed uniques ( 67 ) is the amount of different mixtures.\n",
            "\n",
            "\n",
            "Total concentrations measured\n",
            "------------------------------\n",
            "The dataset does not contain null c_total values.\n",
            "     Total concentration value greater than 0: 222\n",
            "     Total concentration value 0: 3\n",
            "As mentioned in the \"Metal ion concentration data\" lecture by Perez, \n",
            "a value of 0 can be assumed to be a result indicating pure tap water. \n",
            "Thus, the measured amount of total concentrations is 225\n",
            "\n",
            "\n",
            "More info on duplicate data\n",
            "------------------------------\n",
            "Mixtures with less than 4 duplicates: 43\n",
            "Mixtures with 4 or more duplicates: 24\n",
            "\n",
            "\n",
            "Mixtures with 4 or more duplicates are:\n",
            "\n",
            " c_total     Cd     Pb  duplicates\n",
            "      50    0.0   50.0           4\n",
            "      50   10.0   40.0           4\n",
            "      50   20.0   30.0           4\n",
            "      50   30.0   20.0           4\n",
            "      50   40.0   10.0           4\n",
            "      50   50.0    0.0           4\n",
            "      70    0.0   70.0           4\n",
            "      70   14.0   56.0           4\n",
            "      70   28.0   42.0           4\n",
            "      70   42.0   28.0           4\n",
            "      70   56.0   14.0           4\n",
            "      70   70.0    0.0           4\n",
            "     100    0.0  100.0           4\n",
            "     100   20.0   80.0           4\n",
            "     100   40.0   60.0           4\n",
            "     100   60.0   40.0           4\n",
            "     100   80.0   20.0           4\n",
            "     100  100.0    0.0           4\n",
            "     200    0.0  200.0           4\n",
            "     200   40.0  160.0           4\n",
            "     200   80.0  120.0           4\n",
            "     200  120.0   80.0           4\n",
            "     200  160.0   40.0           4\n",
            "     200  200.0    0.0           4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VAqrJvlXOIA"
      },
      "source": [
        "## Standardization of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLM1lpkNXOIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e973eac-d715-494e-bb53-e5b596cc95be"
      },
      "source": [
        "#Standardize the dataset features by removing the mean and scaling to unit variance. \n",
        "#In other words, use z-score to scale the dataset features (Mod1, Mod2, Mod3)\n",
        "def df_z_standardizer(df,method=False):\n",
        "  \n",
        "  standardized_df = df.copy()\n",
        "  if not method:\n",
        "    # Go through columns and transform the values by \n",
        "    # removing mean and divide with standard variance.\n",
        "    for column in standardized_df.columns:\n",
        "        standardized_df[column] = (standardized_df[column] -\n",
        "                                   standardized_df[column].mean()) / standardized_df[column].std()\n",
        "  elif method == 'stats':\n",
        "    scaled = stats.zscore(df)\n",
        "    #scaler = StandardScaler()\n",
        "    #scaled = scaler.fit_transform(standardized_df)\n",
        "    standardized_df = pd.DataFrame(scaled, columns=['Mod1','Mod2','Mod3'])\n",
        "\n",
        "  elif method == 'scaler':\n",
        "    scaler = StandardScaler()\n",
        "    scaled = scaler.fit_transform(standardized_df)\n",
        "    standardized_df = pd.DataFrame(scaled, columns=['Mod1','Mod2','Mod3'])\n",
        "    \n",
        "  return standardized_df\n",
        "\n",
        "# Build a new dataframe with standardized feature values\n",
        "water_data_features = water_data[['Mod1','Mod2','Mod3']]\n",
        "z_std_features = df_z_standardizer(water_data_features,'scaler')\n",
        "\n",
        "standardized_water_data = water_data.drop(columns=['Mod1','Mod2','Mod3'])\n",
        "standardized_water_data = pd.concat([standardized_water_data,z_std_features],\n",
        "                                    axis=1)\n",
        "\n",
        "# Creating datasets to use in evaluations\n",
        "water_data_x_y = standardized_water_data\n",
        "# Print the 5 first samples (i.e. rows) of the scaled dataset\n",
        "print(standardized_water_data.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   c_total     Cd      Pb      Mod1      Mod2      Mod3\n",
            "0     2000  800.0  1200.0  0.166505 -0.508756 -1.499041\n",
            "1       35   14.0    21.0 -0.892616 -0.701641  0.685861\n",
            "2       35   14.0    21.0 -0.852896 -0.701806  1.065760\n",
            "3       35   35.0     0.0 -0.040629 -0.643767  0.584863\n",
            "4      100   20.0    80.0 -0.520568 -0.276268 -0.058789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_81-CWGXOIB"
      },
      "source": [
        "## C-index code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtkswsfcXOIB"
      },
      "source": [
        "def cindex(true, pred, regression=False,silent=False):\n",
        "  n = 0\n",
        "  n_total = 0\n",
        "  pairs = list()\n",
        "  neg_pairs = list()\n",
        "  pos_pairs = list()\n",
        "\n",
        "  # Create true,prediction -pairs\n",
        "  for i in range(len(true)):\n",
        "    pairs.append([true[i],pred[i]])\n",
        "  \n",
        "  if not regression:\n",
        "    # this only works with \"binary\"\n",
        "    for i in range(len(pairs)):\n",
        "      if pairs[i][0] != 1:\n",
        "        neg_pairs.append(pairs[i])\n",
        "      else:\n",
        "        pos_pairs.append(pairs[i])\n",
        "    for i in range(len(neg_pairs)):\n",
        "      for j in range(len(pos_pairs)):\n",
        "        test_one = neg_pairs[i][0]>pos_pairs[j][0]\n",
        "        test_two = neg_pairs[i][1]>pos_pairs[j][1]\n",
        "        test_three = neg_pairs[i][1]==pos_pairs[j][1]\n",
        "\n",
        "        if test_one == test_two:\n",
        "          n += 1\n",
        "          if test_three:\n",
        "            n -= 0.5\n",
        "        n_total += 1\n",
        "  else:\n",
        "    # this should apply to regression cases\n",
        "    for i in range(len(pairs)):\n",
        "      for j in range(i+1,len(pairs)):\n",
        "        if pairs[i][0] != pairs[j][0]:\n",
        "          test_one = pairs[i][0]<pairs[j][0]\n",
        "          test_two = pairs[i][1]<pairs[j][1]\n",
        "          test_three = pairs[i][1]==pairs[j][1]\n",
        "          n_total += 1\n",
        "          if test_one == test_two:\n",
        "            n += 1\n",
        "            if test_three:\n",
        "              n -= 0.5\n",
        "  cindx = n / n_total\n",
        "\n",
        "  if not silent:\n",
        "    print('CONCORDANCE INDEX RESULTS')\n",
        "    print('-'*35)\n",
        "    print('N:',n)\n",
        "    print('N total:',n_total)\n",
        "    print('C-INDEX:',cindx)\n",
        "  \n",
        "  return cindx"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF17fYTtXOIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54752caa-da53-4f7e-f47b-91a5e97af7f4"
      },
      "source": [
        "#test cindex function with following values\n",
        "true_labels = [-1, 1, 1, -1, 1]\n",
        "predictions = [0.60, 0.80, 0.75, 0.75, 0.70]\n",
        "cindx = cindex(true_labels, predictions)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONCORDANCE INDEX RESULTS\n",
            "-----------------------------------\n",
            "N: 4.5\n",
            "N total: 6\n",
            "C-INDEX: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wX9Zxw3PT3X",
        "outputId": "9406d30a-df2a-44a6-89a9-4af0cc70b70a"
      },
      "source": [
        "# Testing with the values from C-index in regression case example (Concordance_index_in_general_case.ppt)\r\n",
        "true_data_points = [7.1,2.3,5.4,1.8]\r\n",
        "pred_data_points = [7.4,3.4,8.5,3.9]\r\n",
        "cindex(true_data_points,pred_data_points,True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONCORDANCE INDEX RESULTS\n",
            "-----------------------------------\n",
            "N: 4\n",
            "N total: 6\n",
            "C-INDEX: 0.6666666666666666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfpYJLwbXOIB"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ6aFl5ZXOIB"
      },
      "source": [
        "# Leave-One-Out CV\r\n",
        "def LOO_cross_validation(dataset, neighbours_n = 3):\r\n",
        "  folds = list(dataset)\r\n",
        "  cindex_ctotal_arr = list()\r\n",
        "  cindex_cd_arr = list()\r\n",
        "  cindex_pb_arr = list()\r\n",
        "  predictions = list()\r\n",
        "  true_labels = list()\r\n",
        "  knn = KNeighborsRegressor(n_neighbors=neighbours_n)\r\n",
        "\r\n",
        "  # Splitting folds and data -- in this case iterating the dataset\r\n",
        "  for i in range(len(folds)):\r\n",
        "    # Fit with all but the index\r\n",
        "    train_set = list(folds)\r\n",
        "    test_set = train_set.pop(i)\r\n",
        "    y_train = [arr[:3] for arr in train_set]\r\n",
        "    x_train = [arr[3:] for arr in train_set]\r\n",
        "\r\n",
        "\r\n",
        "    y_train_nparr = np.asarray(y_train)\r\n",
        "    x_train_nparr = np.asarray(x_train)\r\n",
        "\r\n",
        "    knn.fit(x_train, y_train_nparr)\r\n",
        "    # Test with the index\r\n",
        "    y_test = np.array([test_set[0],test_set[1],test_set[2]])\r\n",
        "    x_test = np.array([test_set[3],test_set[4],test_set[5]])\r\n",
        "    # Gather predictions and true labels\r\n",
        "    pred = knn.predict(x_test.reshape(1,-1))\r\n",
        "    \r\n",
        "    predictions.append(pred[0])\r\n",
        "    true_labels.append(y_test)\r\n",
        "\r\n",
        "  temp_ctotal_arr = list()\r\n",
        "  temp_ctotal_pred_arr = list()\r\n",
        "  temp_cd_arr = list()\r\n",
        "  temp_cd_pred_arr = list()\r\n",
        "  temp_pb_arr = list()\r\n",
        "  temp_pb_pred_arr = list()\r\n",
        "  \r\n",
        "  for i in range(len(true_labels)):\r\n",
        "    temp_ctotal_arr.append(true_labels[i][0])\r\n",
        "    temp_ctotal_pred_arr.append(predictions[i][0])\r\n",
        "    temp_cd_arr.append(true_labels[i][1])\r\n",
        "    temp_cd_pred_arr.append(predictions[i][1])\r\n",
        "    temp_pb_arr.append(true_labels[i][2])\r\n",
        "    temp_pb_pred_arr.append(predictions[i][2])\r\n",
        "  \r\n",
        "  cindex_ctotal_arr.append(temp_ctotal_arr)\r\n",
        "  cindex_ctotal_arr.append(temp_ctotal_pred_arr)\r\n",
        "\r\n",
        "  cindex_cd_arr.append(temp_cd_arr)\r\n",
        "  cindex_cd_arr.append(temp_cd_pred_arr)\r\n",
        "\r\n",
        "  cindex_pb_arr.append(temp_pb_arr)\r\n",
        "  cindex_pb_arr.append(temp_pb_pred_arr)\r\n",
        "\r\n",
        "  cindex_ctotal = cindex(cindex_ctotal_arr[0],cindex_ctotal_arr[1],True,True)\r\n",
        "  cindex_cd = cindex(cindex_cd_arr[0],cindex_cd_arr[1],True,True)\r\n",
        "  cindex_pb = cindex(cindex_pb_arr[0],cindex_pb_arr[1],True,True)\r\n",
        "\r\n",
        "  print('LEAVE-ONE-OUT CROSS-VALIDATION RESULTS')\r\n",
        "  print('-'*42)\r\n",
        "  print('C-index results:')\r\n",
        "  print('   c_total:',cindex_ctotal)\r\n",
        "  print('   cd:',cindex_cd)\r\n",
        "  print('   pb:',cindex_pb)\r\n",
        "  print('-'*42)\r\n",
        "\r\n",
        "# Leave-Replicas-Out split\r\n",
        "def LRO_split(dataset):\r\n",
        "  dataset_split = list()\r\n",
        "  temp_list = list()\r\n",
        "  dataset_copy = dataset\r\n",
        "  # Sort the dataset based on Cd and Pb columns so the groups are sorted as a result\r\n",
        "  grouped_data = dataset_copy.sort_values(['Cd','Pb'],\r\n",
        "                                          ascending=[True, False]).values\r\n",
        "  # Go through the sorted data and form the splits based on groups\r\n",
        "  for i in range(len(grouped_data)):\r\n",
        "    if not temp_list:\r\n",
        "      temp_list.append(grouped_data[i])\r\n",
        "      continue\r\n",
        "    if temp_list[0][1] == grouped_data[i][1] and temp_list[0][2] == grouped_data[i][2]:\r\n",
        "      temp_list.append(grouped_data[i])\r\n",
        "    else:\r\n",
        "      dataset_split.append(temp_list)\r\n",
        "      temp_list = list()\r\n",
        "      temp_list.append(grouped_data[i])\r\n",
        "  dataset_split.append(temp_list)\r\n",
        "\r\n",
        "  return dataset_split\r\n",
        "\r\n",
        "# Leave-Replicas-Out CV\r\n",
        "def LRO_cross_validation(dataset, neighbours_n = 3):\r\n",
        "  # Form folds by splitting the data with LRO_split()\r\n",
        "  folds = LRO_split(dataset)\r\n",
        "  cindex_arr = list()\r\n",
        "  knn = KNeighborsRegressor(n_neighbors = neighbours_n)\r\n",
        "  true_labels = list()\r\n",
        "  predictions = list()\r\n",
        "  cindex_ctotal_arr = list()\r\n",
        "  cindex_cd_arr = list()\r\n",
        "  cindex_pb_arr = list()\r\n",
        "\r\n",
        "  # Iterate folds to perform the train and test split group by group\r\n",
        "  for i in range(len(folds)):\r\n",
        "    train_set_temp = list(folds)\r\n",
        "    train_set = list()\r\n",
        "    test_set = train_set_temp.pop(i)\r\n",
        "    for j in range(len(train_set_temp)):\r\n",
        "      for k in range(len(train_set_temp[j])):\r\n",
        "        train_set.append(train_set_temp[j][k])\r\n",
        "    y_train = [arr[:3] for arr in train_set]\r\n",
        "    x_train = [arr[3:] for arr in train_set]\r\n",
        "    knn.fit(x_train, y_train)\r\n",
        "\r\n",
        "    # Test with the index group\r\n",
        "    prediction_mean = list()\r\n",
        "    for j in range(len(test_set)):\r\n",
        "      y_test = np.array([test_set[j][0],test_set[j][1],test_set[j][2]])\r\n",
        "      x_test = np.array([test_set[j][3],test_set[j][4],test_set[j][5]])\r\n",
        "      \r\n",
        "      # Gather predictions and true labels\r\n",
        "      pred = knn.predict(x_test.reshape(1,-1))\r\n",
        "      predictions.append(pred[0])\r\n",
        "      true_labels.append(np.array([test_set[0][0],test_set[0][1],test_set[0][2]]))\r\n",
        "\r\n",
        "  temp_ctotal_arr = list()\r\n",
        "  temp_ctotal_pred_arr = list()\r\n",
        "  temp_cd_arr = list()\r\n",
        "  temp_cd_pred_arr = list()\r\n",
        "  temp_pb_arr = list()\r\n",
        "  temp_pb_pred_arr = list()\r\n",
        "  \r\n",
        "  for i in range(len(true_labels)):\r\n",
        "    temp_ctotal_arr.append(true_labels[i][0])\r\n",
        "    temp_ctotal_pred_arr.append(predictions[i][0])\r\n",
        "    temp_cd_arr.append(true_labels[i][1])\r\n",
        "    temp_cd_pred_arr.append(predictions[i][1])\r\n",
        "    temp_pb_arr.append(true_labels[i][2])\r\n",
        "    temp_pb_pred_arr.append(predictions[i][2])\r\n",
        "  \r\n",
        "  cindex_ctotal_arr.append(temp_ctotal_arr)\r\n",
        "  cindex_ctotal_arr.append(temp_ctotal_pred_arr)\r\n",
        "\r\n",
        "  cindex_cd_arr.append(temp_cd_arr)\r\n",
        "  cindex_cd_arr.append(temp_cd_pred_arr)\r\n",
        "\r\n",
        "  cindex_pb_arr.append(temp_pb_arr)\r\n",
        "  cindex_pb_arr.append(temp_pb_pred_arr)\r\n",
        "\r\n",
        "  cindex_ctotal = cindex(cindex_ctotal_arr[0],cindex_ctotal_arr[1],True,True)\r\n",
        "  cindex_cd = cindex(cindex_cd_arr[0],cindex_cd_arr[1],True,True)\r\n",
        "  cindex_pb = cindex(cindex_pb_arr[0],cindex_pb_arr[1],True,True)\r\n",
        "\r\n",
        "  print('LEAVE-REPLICAS-OUT CROSS-VALIDATION RESULTS')\r\n",
        "  print('-'*42)\r\n",
        "  print('C-index results:')\r\n",
        "  print('   c_total:',cindex_ctotal)\r\n",
        "  print('   cd:',cindex_cd)\r\n",
        "  print('   pb:',cindex_pb)\r\n",
        "  print('-'*42)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgsMboHmXOIC"
      },
      "source": [
        "## Results for Leave-One-Out cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1BIWyFIXOIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c3ad5d-1b62-4b90-ae20-a0afd2c266a1"
      },
      "source": [
        "#In this cell run your code for leave-One-Out cross-validation and print the corresponding results.\r\n",
        "LOO_cross_validation(standardized_water_data.values)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LEAVE-ONE-OUT CROSS-VALIDATION RESULTS\n",
            "------------------------------------------\n",
            "C-index results:\n",
            "   c_total: 0.911953783337677\n",
            "   cd: 0.8983588758508373\n",
            "   pb: 0.8731782686766609\n",
            "------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1yNcGHaXOIC"
      },
      "source": [
        "## Results for Leave-Replicas-Out cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ9MgPrCXOIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce073a9b-e55c-435a-eec2-00e953722d77"
      },
      "source": [
        "#In this cell run your script for leave-Replicas-Out cross-validation and print the corresponding results.\r\n",
        "LRO_cross_validation(standardized_water_data)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LEAVE-REPLICAS-OUT CROSS-VALIDATION RESULTS\n",
            "------------------------------------------\n",
            "C-index results:\n",
            "   c_total: 0.814025714533924\n",
            "   cd: 0.7578402305090408\n",
            "   pb: 0.7675491710861486\n",
            "------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V_wjuUqXOIC"
      },
      "source": [
        "## Interpretation of results\n",
        "#### Answer the following questions based on the results obtained\n",
        "- Which cross-validation approach had more optimistic results?\n",
        "- Which cross-validation generalize better on unseen data? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi-SvSRZ1f_N"
      },
      "source": [
        "#In this cell write your answers to the questions about Interpretation of Results.\r\n",
        "\r\n",
        "<p>Regarding <br/>\r\n",
        "<b>C total:</b> LOOCV was more optimistic with a cindex of ~0.912 vs LROCV with ~0.814<br/>\r\n",
        "<b>Cd:</b> LOOCV was more optimistic with a cindex of ~0.898 vs LROCV with ~0.758<br/>\r\n",
        "<b>Pb:</b> LOOCV was more optimistic with a cindex of ~0.873 vs LROCV with ~0.768<br/></p>\r\n",
        "<p>Overall the LOOCV seems to be the more optimistic one.</p>\r\n",
        "\r\n",
        "<p>Most likely the LROCV is the one to better generalize itself on unseen data. Scoring-wise LOOCV may benefit from replicates in the data which causes data leakage between test and training sets and thus one could argue that in this case it would be - if not absolutely correct then at least more realistic with the given results - to assume that the LROCV scoring tells the truth about its generalizing ability moreso than LOOCV.</p>"
      ]
    }
  ]
}